{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import concurrent.futures\n",
    "import itertools\n",
    "print(\"Hello World\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting image to tiles and store into different folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Ignore all FutureWarning warnings that might flood the console log\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# from encryption import encode, decode\n",
    "\n",
    "import time\n",
    "from utils import create_patches_v3, create_binary_mask, create_patches_v2, create_patches_v1, create_patches_coordinates, create_patches_coordinates_v1\n",
    "from scipy.stats.contingency import crosstab\n",
    "from sklearn.metrics import mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "\n",
    "# By default, the number of sub datasets is set as 3\n",
    "# N_folders = 3\n",
    "\n",
    "# Loading directory\n",
    "wsi_dir = \"/root/code2024/dataset/4WSI/data/\"\n",
    "patch_size = 224\n",
    "cpu_core = 12\n",
    "\n",
    "# Saving directory\n",
    "save_dir = wsi_dir\n",
    "\n",
    "downsize = patch_size\n",
    "patch_extraction_size = patch_size\n",
    "mask_overlap = 80.0\n",
    "batch_size = 64\n",
    "cpu_workers = cpu_core\n",
    "use_prob_threshold = 0.8 # None  # whether to give final prediction {0,1} based on certain probability\n",
    "\n",
    "# torch.manual_seed(250)\n",
    "print('--------------Start--------------')\n",
    "# read the files\n",
    "wsi_files = os.listdir(wsi_dir)\n",
    "# get all files except temp directory containing patches\n",
    "wsi_files = [f for f in wsi_files if f.endswith(\"svs\") or f.endswith(\"ndpi\") or f.endswith(\"mrxs\")]\n",
    "\n",
    "print(f\"Total files in {wsi_dir} directory are {len(wsi_files)}\")\n",
    "\n",
    "path = os.path.join(wsi_dir, \"cnn_ensemble_updated\")\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "lst_patching_time = []\n",
    "\n",
    "stat_NMI_graph = [] \n",
    "stat_NMI_base = []\n",
    "coords = []\n",
    "new_file_paths = []\n",
    "# start patching process over a certain WSI\n",
    "for i, f in enumerate(wsi_files):\n",
    "\n",
    "    st_binarymask = time.time()\n",
    "    new_file_path = os.path.join(path, f.split(\".\")[0])\n",
    "    print(new_file_path)\n",
    "\n",
    "\n",
    "    # just take the name not extension\n",
    "    if not os.path.exists(new_file_path):\n",
    "        os.mkdir(new_file_path)\n",
    "        \n",
    "    # find binary mask to locate tissue on WSI            \n",
    "    w, h = create_binary_mask(wsi_dir, f, new_file_path, downsize=patch_size)\n",
    "\n",
    "    # print(w/224, h/224)\n",
    "    et_binarymask = time.time()\n",
    "\n",
    "    binarymask_processtime = et_binarymask - st_binarymask\n",
    "    print(f\"Time spent on create_binary_mask is {binarymask_processtime} for {f}\")\n",
    "        \n",
    "    st_patching = time.time()\n",
    "    patch_folder = os.path.join(new_file_path, \"patches\")\n",
    "    \n",
    "    if not os.path.exists(patch_folder):\n",
    "    # # else:\n",
    "        os.mkdir(patch_folder) \n",
    "    if os.path.exists(patch_folder):\n",
    "    #     os.rmdir(patch_folder)\n",
    "    # elif not os.path.exists(patch_folder):\n",
    "    # # else:\n",
    "    #     os.mkdir(patch_folder) \n",
    "        # coordinates, id_to_coordinates = create_patches_coordinates(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #单进程\n",
    "        # coordinates, id_to_coordinates, patches = create_patches_v1(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #单进程\n",
    "        \n",
    "        # create_patches_v3(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        \n",
    "        # 1. create coordinates with multi-processing\n",
    "        res = create_patches_coordinates_v1(wsi_dir, f, new_file_path, workers=cpu_workers, patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        \n",
    "        # patches = create_patches_v2(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "            # patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        # print(len(coordinates))\n",
    "        # print(len(patches))\n",
    "        # print(id_to_coordinates)\n",
    "\n",
    "        # print(\"Coordinates Done!\", len(res), res[:3])\n",
    "        # coordinates = list(res.values())\n",
    "        coordinates = res.values()\n",
    "        ids = res.keys()\n",
    "        # print(\"Coordinates Done!\", len(res), len(coordinates), coordinates[:3])\n",
    "        coords.append(coordinates)\n",
    "        new_file_paths.append(new_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decrypting coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = res.values()\n",
    "# ids = res.keys()\n",
    "print(len(coords), len(new_file_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "os.mkdir(graph_split_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-based Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create sub data sets with splitting algorithm\n",
    "from graph_based_splitting import graph_coloring_based_splitting, graph_coloring_based_splitting_visual, graph_coloring_based_splitting_visual1\n",
    "\n",
    "# If ``strategy`` is a string, it must be one of the following,\n",
    "# each of which represents one of the built-in strategy functions.\n",
    "\n",
    "# * ``'largest_first'``\n",
    "# * ``'random_sequential'``\n",
    "# * ``'smallest_last'``\n",
    "# * ``'independent_set'``\n",
    "# * ``'connected_sequential_bfs'``\n",
    "# * ``'connected_sequential_dfs'``\n",
    "# * ``'connected_sequential'`` (alias for the previous strategy)\n",
    "# * ``'saturation_largest_first'``\n",
    "# * ``'DSATUR'`` (alias for the previous strategy)\n",
    "# graph_distribution = []\n",
    "# graph_split_file = os.mkdir(os.path.join(new_file_path, 'graph_split/'))\n",
    "\n",
    "for coordinates, new_file_path in zip(coords, new_file_paths): \n",
    "    graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "    if not os.path.exists(graph_split_file):\n",
    "        os.mkdir(graph_split_file)\n",
    "    \n",
    "    strategies = ['saturation_largest_first', 'largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "            'connected_sequential']\n",
    "    \n",
    "    # strategies = ['independent_set', 'connected_sequential']\n",
    "    for my_strategy in strategies: \n",
    "        # d, dataset_number = graph_coloring_based_splitting1(coordinates, patch_size, strategy=my_strategy)     # graph\n",
    "        d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "\n",
    "        # Specify the directory path\n",
    "        indexes = [item[0] for item in d.items()]\n",
    "        patch_folder = f'{new_file_path}/patches/'\n",
    "        patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "        # Create a DataFrame from the list of file names\n",
    "        df = pd.DataFrame({\"Index\": indexes, \"PatchName\": patch_names})\n",
    "\n",
    "        # Specify the CSV file path where the DataFrame will be saved\n",
    "        csv_file_path = f'{new_file_path}/patch_names_{my_strategy}.csv'\n",
    "\n",
    "        # Save the DataFrame to a CSV file\n",
    "        df.to_csv(csv_file_path, index=False)\n",
    "        print(f'File names have been saved to {csv_file_path}')\n",
    "\n",
    "        OrgPatchName = df['PatchName']\n",
    "\n",
    "        # store the split image names into different .csv files\n",
    "        list = []\n",
    "        for j in range(dataset_number):\n",
    "            \n",
    "            indexes = []\n",
    "            names = []\n",
    "            for item in d.items(): \n",
    "                # print(item) \n",
    "                if item[1] == j:\n",
    "                    index = item[0]\n",
    "                    # print(i, index, OrgPatchName[index])\n",
    "                    indexes.append(index)\n",
    "                    names.append(OrgPatchName[index])\n",
    "            \n",
    "            dataframe = pd.DataFrame({\"Index\": indexes, \"PatchName\": names})\n",
    "            destfp = os.path.join(graph_split_file, f'graph_patchesnames_'+str(j)+my_strategy+'.csv')\n",
    "            print(destfp)\n",
    "            # dataframe.to_csv('patchesnames_'+str(i)+'.csv', index=False, sep=',')\n",
    "            dataframe.to_csv(destfp, index=False, sep=',')\n",
    "\n",
    "    print(\"---------------Well Done----------------\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evenly splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(new_file_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_splitting import shuffle_evenly_division\n",
    "\n",
    "# Evenly partitionly #\n",
    "# * ``is_shuffle =  True``\n",
    "# * ``is_shuffle =  False``\n",
    "flag = True\n",
    "dataset_number = 4\n",
    "for coordinates, new_file_path in zip(coords, new_file_paths): \n",
    "    baseline_split_file = f'{new_file_path}/even_split/'\n",
    "    if not os.path.exists(baseline_split_file):\n",
    "        os.mkdir(baseline_split_file)\n",
    "    \n",
    "    # os.mkdir(graph_split_file)\n",
    "    # res_path ='/root/code2024/dataset/2WSI/data/cnn_ensemble_updated/TCGA-A7-A6VX-01Z-00-DX2/even_split/'\n",
    "    # patch_folder = '/root/code2024/dataset/2WSI/data/cnn_ensemble_updated/TCGA-A7-A6VX-01Z-00-DX2/patches/'\n",
    "    patch_folder = f'{new_file_path}/patches/'\n",
    "    patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "    # print(res_path)\n",
    "    print(coordinates)\n",
    "    # print(res.keys())\n",
    "    \n",
    "    # custom_keys = [key.replace(',', '_')+'.png' for key in res.keys()]\n",
    "\n",
    "    \n",
    "    is_shuffled = f'is_shuffled_{flag}'\n",
    "    base_distribution = shuffle_evenly_division(patch_names, dataset_number, is_shuffle = flag)\n",
    "\n",
    "    base_coords = shuffle_evenly_division(coordinates, dataset_number, is_shuffle = flag)\n",
    "    print(base_coords)\n",
    "    # for i in base_coords:\n",
    "        # print(i)\n",
    "    # indexes = [item[0] for item in d.items()]\n",
    "    # patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "    list = []\n",
    "    for j, item in enumerate(base_distribution): \n",
    "        indexes = []\n",
    "        names = []\n",
    "        print(item) \n",
    "        # for item in df['PatchName']:\n",
    "\n",
    "        indexes = [i for i in range(len(item))]\n",
    "        patch_names = [os.path.join(patch_folder, patch) for patch in item]\n",
    "\n",
    "        dataframe = pd.DataFrame({\"Indexes\": indexes, \"PatchName\": patch_names})\n",
    "        \n",
    "        destfp = os.path.join(baseline_split_file, f'even_patchesnames_'+str(j)+f'_{is_shuffled}.csv')\n",
    "        print(destfp)\n",
    "        # dataframe.to_csv('patchesnames_'+str(i)+'.csv', index=False, sep=',')\n",
    "        dataframe.to_csv(destfp, index=False, sep=',')\n",
    "\n",
    "print(\"---------------Well Done----------------\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate the hist \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 加载图像\n",
    "# image = cv2.imread(f'/root/code2024/dataset/2WSI/data/cnn_ensemble_updated/TCGA-A7-A6VX-01Z-00-DX2/patches/TCGA-A7-A6VX-01Z-00-DX2_28560_6720.png')\n",
    "# image = cv2.imread(f'/root/code2024/dataset/3WSI/data/cnn_ensemble_updated/TCGA-A7-A6VX-01Z-00-DX2/patches/TCGA-A7-A6VX-01Z-00-DX2_28560_6720.png')\n",
    "image_path = '/root/code2024/dataset/tWSI/data/cnn_ensemble_updated/TCGA-AC-A2FB-01Z-00-DX1/patches/TCGA-AC-A2FB-01Z-00-DX1_336_7728.png' \n",
    "image = cv2.imread(image_path)\n",
    "# 分离颜色通道\n",
    "channels = cv2.split(image)\n",
    "\n",
    "# 颜色通道标签\n",
    "colors = ('b', 'g', 'r')\n",
    "\n",
    "# 创建颜色直方图\n",
    "plt.figure()\n",
    "plt.title('Color Histogram')\n",
    "plt.xlabel('Bin')\n",
    "plt.ylabel('# of Pixels')\n",
    "\n",
    "# 遍历每个颜色通道\n",
    "for (channel, color) in zip(channels, colors):\n",
    "    # 计算该通道的直方图\n",
    "    hist = cv2.calcHist([channel], [0], None, [256], [0, 256])\n",
    "    \n",
    "    # 绘制直方图\n",
    "    plt.plot(hist, color=color)\n",
    "    plt.xlim([0, 256])\n",
    "\n",
    "# 显示直方图\n",
    "# plt.savefig('./hist2.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Splitting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
