{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Hello World!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encrypting Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Ignore all FutureWarning warnings that might flood the console log\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "\n",
    "# from encryption import encode, decode\n",
    "\n",
    "import time\n",
    "from utils import create_patches_v3, create_binary_mask, create_patches_v2, create_patches_v1, create_patches_coordinates, create_patches_coordinates_v1\n",
    "\n",
    "# Loading directory\n",
    "wsi_dir = \"/root/code2024/dataset/1WSI/data/\"\n",
    "patch_size = 224\n",
    "cpu_core = 12\n",
    "\n",
    "# Saving directory\n",
    "save_dir = wsi_dir\n",
    "\n",
    "downsize = patch_size\n",
    "patch_extraction_size = patch_size\n",
    "mask_overlap = 80.0\n",
    "batch_size = 64\n",
    "cpu_workers = cpu_core\n",
    "use_prob_threshold = 0.8 # None  # whether to give final prediction {0,1} based on certain probability\n",
    "\n",
    "# torch.manual_seed(250)\n",
    "print('--------------Start--------------')\n",
    "# read the files\n",
    "wsi_files = os.listdir(wsi_dir)\n",
    "# get all files except temp directory containing patches\n",
    "wsi_files = [f for f in wsi_files if f.endswith(\"svs\") or f.endswith(\"ndpi\") or f.endswith(\"mrxs\")]\n",
    "\n",
    "print(f\"Total files in {wsi_dir} directory are {len(wsi_files)}\")\n",
    "\n",
    "path = os.path.join(wsi_dir, \"cnn_ensemble_updated\")\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "lst_patching_time = []\n",
    "\n",
    "stat_NMI_graph = [] \n",
    "stat_NMI_base = []\n",
    "coords = []\n",
    "new_file_paths = []\n",
    "# start patching process over a certain WSI\n",
    "for i, f in enumerate(wsi_files):\n",
    "\n",
    "    st_binarymask = time.time()\n",
    "    new_file_path = os.path.join(path, f.split(\".\")[0])\n",
    "    print(new_file_path)\n",
    "\n",
    "    # just take the name not extension\n",
    "    if not os.path.exists(new_file_path):\n",
    "        os.mkdir(new_file_path)\n",
    "        \n",
    "    # find binary mask to locate tissue on WSI            \n",
    "    w, h = create_binary_mask(wsi_dir, f, new_file_path, downsize=patch_size)\n",
    "\n",
    "    # print(w/224, h/224)\n",
    "    et_binarymask = time.time()\n",
    "\n",
    "    binarymask_processtime = et_binarymask - st_binarymask\n",
    "    print(f\"Time spent on create_binary_mask is {binarymask_processtime} for {f}\")\n",
    "        \n",
    "    st_patching = time.time()\n",
    "    patch_folder = os.path.join(new_file_path, \"patches\")\n",
    "    \n",
    "    if not os.path.exists(patch_folder):\n",
    "    # # else:\n",
    "        os.mkdir(patch_folder) \n",
    "    if os.path.exists(patch_folder):\n",
    "    #     os.rmdir(patch_folder)\n",
    "    # elif not os.path.exists(patch_folder):\n",
    "    # # else:\n",
    "    #     os.mkdir(patch_folder) \n",
    "        # coordinates, id_to_coordinates = create_patches_coordinates(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #单进程\n",
    "        # coordinates, id_to_coordinates, patches = create_patches_v1(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #单进程\n",
    "        \n",
    "        # create_patches_v3(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        \n",
    "        # 1. create coordinates with multi-processing\n",
    "        res = create_patches_coordinates_v1(wsi_dir, f, new_file_path, workers=cpu_workers, patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        \n",
    "        # patches = create_patches_v2(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "            # patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        # print(len(coordinates))\n",
    "        # print(len(patches))\n",
    "        # print(id_to_coordinates)\n",
    "\n",
    "        # print(\"Coordinates Done!\", len(res), res[:3])\n",
    "        # coordinates = list(res.values())\n",
    "        coordinates = res.values()\n",
    "        ids = res.keys()\n",
    "        # print(\"Coordinates Done!\", len(res), len(coordinates), coordinates[:3])\n",
    "        coords.append(coordinates)\n",
    "        new_file_paths.append(new_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_file_paths)\n",
    "print(len(coords))\n",
    "# import networkx as nx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph-based Split Data Pertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "def count_files_with_numeric_pattern(start_path, file_pattern):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        for file in files:\n",
    "            if fnmatch.fnmatch(file, file_pattern):\n",
    "                count += 1\n",
    "                # print(os.path.join(root, file))  # 打印匹配的文件路径\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from encryption_r import encode, decode\n",
    "    # r coordinates, new_file_path in zip(coords, new_file_paths): \n",
    "  \n",
    "for new_file_path in new_file_paths: \n",
    "    graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "    os.listdir(graph_split_file)\n",
    "    strategies = ['largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "            'connected_sequential', 'saturation_largest_first']\n",
    "    for my_strategy in strategies: \n",
    "        # my_strategy = 'smallest_last'\n",
    "        # d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "        \n",
    "        # 设置你的目标文件夹名称和文件模式\n",
    "        graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "        file_pattern = f\"*[0-9]{my_strategy}.csv\"\n",
    "\n",
    "        # 计算文件数量\n",
    "        dataset_number = count_files_with_numeric_pattern(graph_split_file, file_pattern)\n",
    "        print(my_strategy, dataset_number)\n",
    "        # print(f\"在 '{target_folder_name}' 文件夹中找到了 {file_count} 个符合 '{file_pattern}' 模式的文件。\")\n",
    "        for j in range(dataset_number):\n",
    "            # sub data set\n",
    "            origfpath = f'{graph_split_file}/graph_patchesnames_{str(j)}{my_strategy}.csv'\n",
    "            encodefpath = f'{graph_split_file}{my_strategy}_R_encode_coordinates_{j}.csv'\n",
    "            decodefpath = f'{graph_split_file}{my_strategy}_R_decode_coordinates_{j}.csv'\n",
    "\n",
    "            print(new_file_path, my_strategy, j)\n",
    "            xencry_list, yencry_list, means, X_centered, Y_centered, x_list, y_list, Rxencry_list, Ryencry_list = encode(origfpath, encodefpath) # type: ignore     \n",
    "            # print(xencry_list, yencry_list, means, X_centered, Y_centered, x_list, y_list, Rxencry_list, Ryencry_list)\n",
    "            xdecry_list, ydecry_list = decode(encodefpath, means, X_centered, Y_centered, decodefpath)\n",
    "\n",
    "            perturbe_df = pd.read_csv(encodefpath)\n",
    "            orig_df = pd.read_csv(origfpath)\n",
    "            estimate_df = pd.read_csv(decodefpath)\n",
    "\n",
    "            PerturbedList = perturbe_df[\"EncryPatchName\"].tolist()\n",
    "            origList = orig_df[\"PatchName\"].tolist()\n",
    "            estList = estimate_df[\"DecryPatchName\"].tolist()\n",
    "            # print(estList)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reconstruct graph with decrypted lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_coordinates import find_decrpytcoordinates, find_encrpytcoordinates\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "from encryption_r import encode, decode\n",
    "    # r coordinates, new_file_path in zip(coords, new_file_paths): \n",
    "\n",
    "de_coords = []    \n",
    "for new_file_path in new_file_paths: \n",
    "    graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "    os.listdir(graph_split_file)\n",
    "    # strategies = ['largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "            # 'connected_sequential', 'saturation_largest_first']\n",
    "    strategies = ['saturation_largest_first']        \n",
    "    for my_strategy in strategies: \n",
    "        # my_strategy = 'smallest_last'\n",
    "        # d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "        \n",
    "        # 设置你的目标文件夹名称和文件模式\n",
    "        graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "        file_pattern = f\"*[0-9]{my_strategy}.csv\"\n",
    "\n",
    "        # 计算文件数量\n",
    "        dataset_number = count_files_with_numeric_pattern(graph_split_file, file_pattern)\n",
    "        print(my_strategy, dataset_number)\n",
    "        # print(f\"在 '{target_folder_name}' 文件夹中找到了 {file_count} 个符合 '{file_pattern}' 模式的文件。\")\n",
    "        coordinates = []\n",
    "        for j in range(dataset_number):\n",
    "            # sub data set\n",
    "            # origfpath = f'{graph_split_file}/graph_patchesnames_{str(j)}{my_strategy}.csv'\n",
    "            # encodefpath = f'{graph_split_file}{my_strategy}_R_encode_coordinates_{j}.csv'\n",
    "            decodefpath = f'{graph_split_file}{my_strategy}_R_decode_coordinates_{j}.csv'\n",
    "\n",
    "            print(new_file_path, my_strategy, j)\n",
    "            # xencry_list, yencry_list, means, X_centered, Y_centered, x_list, y_list, Rxencry_list, Ryencry_list = encode(origfpath, encodefpath) # type: ignore     \n",
    "            # print(xencry_list, yencry_list, means, X_centered, Y_centered, x_list, y_list, Rxencry_list, Ryencry_list)\n",
    "            # xdecry_list, ydecry_list = decode(encodefpath, means, X_centered, Y_centered, decodefpath)\n",
    "\n",
    "            # perturbe_df = pd.read_csv(encodefpath)\n",
    "            # orig_df = pd.read_csv(origfpath)\n",
    "            estimate_df = pd.read_csv(decodefpath)\n",
    "\n",
    "            # PerturbedList = perturbe_df[\"EncryPatchName\"].tolist()\n",
    "            # origList = orig_df[\"PatchName\"].tolist()\n",
    "            estList = estimate_df[\"DecryPatchName\"].tolist()\n",
    "            print(estList)\n",
    "\n",
    "            subcoordinates = [find_decrpytcoordinates(label) for label in estList]\n",
    "            print(subcoordinates)\n",
    "            coordinates.extend(subcoordinates)\n",
    "        de_coords.append(coordinates)\n",
    "\n",
    "# patchName = 'wsi_8064_16800.png'\n",
    "\n",
    "# print(find_decrpytcoordinates(patchName))\n",
    "# print(find_encrpytcoordinates(patchName))\n",
    "print(len(de_coords), de_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create sub data sets with splitting algorithm\n",
    "from graph_based_splitting import graph_coloring_based_splitting, graph_coloring_based_splitting_visual, graph_coloring_based_splitting_visual1\n",
    "\n",
    "# If ``strategy`` is a string, it must be one of the following,\n",
    "# each of which represents one of the built-in strategy functions.\n",
    "\n",
    "# * ``'largest_first'``\n",
    "# * ``'random_sequential'``\n",
    "# * ``'smallest_last'``\n",
    "# * ``'independent_set'``\n",
    "# * ``'connected_sequential_bfs'``\n",
    "# * ``'connected_sequential_dfs'``\n",
    "# * ``'connected_sequential'`` (alias for the previous strategy)\n",
    "# * ``'saturation_largest_first'``\n",
    "# * ``'DSATUR'`` (alias for the previous strategy)\n",
    "# graph_distribution = []\n",
    "# graph_split_file = os.mkdir(os.path.join(new_file_path, 'graph_split/'))\n",
    "\n",
    "for coordinates, new_file_path in zip(de_coords, new_file_paths): \n",
    "    graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "    if not os.path.exists(graph_split_file):\n",
    "        os.mkdir(graph_split_file)\n",
    "    \n",
    "    strategies = ['saturation_largest_first', 'largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "            'connected_sequential']\n",
    "    \n",
    "    # strategies = ['independent_set', 'connected_sequential']\n",
    "    for my_strategy in strategies: \n",
    "        # d, dataset_number = graph_coloring_based_splitting1(coordinates, patch_size, strategy=my_strategy)     # graph\n",
    "        d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "\n",
    "        # # Specify the directory path\n",
    "        # indexes = [item[0] for item in d.items()]\n",
    "        # patch_folder = f'{new_file_path}/patches/'\n",
    "        # patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "        # # Create a DataFrame from the list of file names\n",
    "        # df = pd.DataFrame({\"Index\": indexes, \"PatchName\": patch_names})\n",
    "\n",
    "        # # Specify the CSV file path where the DataFrame will be saved\n",
    "        # csv_file_path = f'{new_file_path}/patch_names_{my_strategy}.csv'\n",
    "\n",
    "        # # Save the DataFrame to a CSV file\n",
    "        # df.to_csv(csv_file_path, index=False)\n",
    "        # print(f'File names have been saved to {csv_file_path}')\n",
    "\n",
    "        # OrgPatchName = df['PatchName']\n",
    "\n",
    "        # # store the split image names into different .csv files\n",
    "        # list = []\n",
    "        # for j in range(dataset_number):\n",
    "            \n",
    "        #     indexes = []\n",
    "        #     names = []\n",
    "        #     for item in d.items(): \n",
    "        #         # print(item) \n",
    "        #         if item[1] == j:\n",
    "        #             index = item[0]\n",
    "        #             # print(i, index, OrgPatchName[index])\n",
    "        #             indexes.append(index)\n",
    "        #             names.append(OrgPatchName[index])\n",
    "            \n",
    "        #     dataframe = pd.DataFrame({\"Index\": indexes, \"PatchName\": names})\n",
    "        #     destfp = os.path.join(graph_split_file, f'graph_patchesnames_'+str(j)+my_strategy+'.csv')\n",
    "        #     print(destfp)\n",
    "        #     # dataframe.to_csv('patchesnames_'+str(i)+'.csv', index=False, sep=',')\n",
    "        #     dataframe.to_csv(destfp, index=False, sep=',')\n",
    "\n",
    "    print(\"---------------Well Done----------------\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evenly splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baseline_splitting import shuffle_evenly_division\n",
    "\n",
    "# Evenly partitionly #\n",
    "# * ``is_shuffle =  True``\n",
    "# * ``is_shuffle =  False``\n",
    "flag = True\n",
    "dataset_numbers = [4, 5, 6, 7, 8]\n",
    "for coords, new_file_path in zip(coordinates, new_file_paths): \n",
    "    for dataset_number in dataset_numbers:\n",
    "        baseline_split_file = f'{new_file_path}/even_split/{dataset_number}/'\n",
    "        if not os.path.exists(baseline_split_file):\n",
    "            os.mkdir(baseline_split_file)\n",
    "    \n",
    "        # os.mkdir(graph_split_file)\n",
    "        # res_path ='/root/code2024/dataset/2WSI/data/cnn_ensemble_updated/TCGA-A7-A6VX-01Z-00-DX2/even_split/'\n",
    "        # patch_folder = '/root/code2024/dataset/2WSI/data/cnn_ensemble_updated/TCGA-A7-A6VX-01Z-00-DX2/patches/'\n",
    "        patch_folder = f'{new_file_path}/patches/'\n",
    "        patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "        # print(res_path)\n",
    "        print(coordinates)\n",
    "        # print(res.keys())\n",
    "        \n",
    "        # custom_keys = [key.replace(',', '_')+'.png' for key in res.keys()]\n",
    "\n",
    "        \n",
    "        is_shuffled = f'is_shuffled_{flag}'\n",
    "        base_distribution = shuffle_evenly_division(patch_names, dataset_number, is_shuffle = flag)\n",
    "\n",
    "        base_coords = shuffle_evenly_division(coordinates, dataset_number, is_shuffle = flag)\n",
    "        print(base_coords)\n",
    "        # for i in base_coords:\n",
    "            # print(i)\n",
    "        # indexes = [item[0] for item in d.items()]\n",
    "        # patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "        list = []\n",
    "        for j, item in enumerate(base_distribution): \n",
    "            indexes = []\n",
    "            names = []\n",
    "            print(item) \n",
    "            # for item in df['PatchName']:\n",
    "\n",
    "            indexes = [i for i in range(len(item))]\n",
    "            patch_names = [os.path.join(patch_folder, patch) for patch in item]\n",
    "\n",
    "            dataframe = pd.DataFrame({\"Indexes\": indexes, \"PatchName\": patch_names})\n",
    "            \n",
    "            destfp = os.path.join(baseline_split_file, f'even_patchesnames_'+str(j)+f'_{is_shuffled}.csv')\n",
    "            print(destfp)\n",
    "            # dataframe.to_csv('patchesnames_'+str(i)+'.csv', index=False, sep=',')\n",
    "            dataframe.to_csv(destfp, index=False, sep=',')\n",
    "\n",
    "print(\"---------------Well Done----------------\")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Even Splitting based Data Pertubation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encryption_r import encode, decode\n",
    "    # r coordinates, new_file_path in zip(coords, new_file_paths): \n",
    "for new_file_path in new_file_paths: \n",
    "    strategy_nos = [4, 5, 6, 7, 8]\n",
    "    for n in strategy_nos:\n",
    "        even_split_file = f'{new_file_path}/even_split/{n}/'\n",
    "        \n",
    "        os.listdir(even_split_file)\n",
    "\n",
    "        strategies = ['is_shuffled_True', 'is_shuffled_False']\n",
    "        for my_strategy in strategies: \n",
    "            # my_strategy = 'smallest_last'\n",
    "            # d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "            \n",
    "            # 设置你的目标文件夹名称和文件模式\n",
    "            # graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "            file_pattern = f\"*[0-9]_{my_strategy}.csv\"\n",
    "\n",
    "            # 计算文件数量\n",
    "            dataset_number = count_files_with_numeric_pattern(even_split_file, file_pattern)\n",
    "\n",
    "            # print(f\"在 '{target_folder_name}' 文件夹中找到了 {file_count} 个符合 '{file_pattern}' 模式的文件。\")\n",
    "            for j in range(dataset_number):\n",
    "                # sub data set\n",
    "                origfpath = f'{even_split_file}/even_patchesnames_{str(j)}_{my_strategy}.csv'\n",
    "                encodefpath = f'{even_split_file}{my_strategy}_R_encode_coordinates_{j}.csv'\n",
    "                decodefpath = f'{even_split_file}{my_strategy}_R_decode_coordinates_{j}.csv'\n",
    "\n",
    "                print(new_file_path, my_strategy, j)\n",
    "                xencry_list, yencry_list, means, X_centered, Y_centered, x_list, y_list, Rxencry_list, Ryencry_list = encode(origfpath, encodefpath) # type: ignore     \n",
    "                # print(xencry_list, yencry_list, means, X_centered, Y_centered, x_list, y_list, Rxencry_list, Ryencry_list)\n",
    "                xdecry_list, ydecry_list = decode(encodefpath, means, X_centered, Y_centered, decodefpath)\n",
    "\n",
    "                perturbe_df = pd.read_csv(encodefpath)\n",
    "                orig_df = pd.read_csv(origfpath)\n",
    "                estimate_df = pd.read_csv(decodefpath)\n",
    "\n",
    "                PerturbedList = perturbe_df[\"EncryPatchName\"].tolist()\n",
    "                origList = orig_df[\"PatchName\"].tolist()\n",
    "                estList = estimate_df[\"DecryPatchName\"].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from parse_coordinates import find_coordinates, find_encrpytcoordinates\n",
    "\n",
    "import os\n",
    "import re\n",
    "\n",
    "simDa = ['graph_split1', 'even_split']\n",
    "x = simDa[1]\n",
    "coordfp = '/root/code2024/pipeline/YD/privacy_eval/coordinates/'\n",
    "\n",
    "for i, new_file_path in enumerate(new_file_paths): \n",
    "    IGx = []\n",
    "    IGy = []\n",
    "    svg_path = ''\n",
    "\n",
    "    encodefpath = []\n",
    "    decodefpath = []\n",
    "    origfpath = []\n",
    "\n",
    "    if x == simDa[0]:\n",
    "        # graph_split1\n",
    "        graph_split_file = f'{new_file_path}/{x}/'\n",
    "        fname = re.split(r'/', new_file_path)[-1]\n",
    "        print(fname)\n",
    "        # os.listdir(graph_split_file)\n",
    "        strategies = ['largest_first', 'random_sequential', 'smallest_last', 'independent_set', 'connected_sequential', 'saturation_largest_first']\n",
    "        for my_strategy in strategies: \n",
    "            file_pattern = f\"*[0-9]{my_strategy}.csv\"\n",
    "            # 计算文件数量\n",
    "            dataset_number = count_files_with_numeric_pattern(graph_split_file, file_pattern)\n",
    "            for j in range(dataset_number):\n",
    "                # sub data set\n",
    "                origfpath = f'{graph_split_file}/graph_patchesnames_{str(j)}{my_strategy}.csv'\n",
    "                encodefpath = f'{graph_split_file}{my_strategy}_R_encode_coordinates_{j}.csv'\n",
    "                decodefpath = f'{graph_split_file}{my_strategy}_R_decode_coordinates_{j}.csv'\n",
    "                svg_path = f'{coordfp}/{x}/{i}{fname}_coords_{my_strategy}_{j}.csv'\n",
    "\n",
    "                perturbe_df = pd.read_csv(encodefpath)\n",
    "                orig_df = pd.read_csv(origfpath)\n",
    "                estimate_df = pd.read_csv(decodefpath)\n",
    "\n",
    "                PerturbedList = perturbe_df[\"EncryPatchName\"].tolist()\n",
    "                origList = orig_df[\"PatchName\"].tolist()\n",
    "                estList = estimate_df[\"DecryPatchName\"].tolist()\n",
    "                print(len(PerturbedList), len(origList), len(estList))\n",
    "\n",
    "                Dx = []\n",
    "                Dy = []\n",
    "                DPx = []\n",
    "                DPy = []\n",
    "                Decx = []\n",
    "                Decy = []\n",
    "                for d, dp, dec in zip(origList, PerturbedList, estList): \n",
    "                    # print(i,c,folder)\n",
    "                    (xorg_coordinate, yorg_coordinate) = find_coordinates(d) # type: ignore\n",
    "                    (x_coordinate, y_coordinate) = find_encrpytcoordinates(dp) # type: ignore\n",
    "                    (xest_coordinate, yest_coordinate) = find_coordinates(dec) # type: ignore\n",
    "                    Dx.append(int(xorg_coordinate))\n",
    "                    Dy.append(int(yorg_coordinate))\n",
    "                    DPx.append(float(x_coordinate))\n",
    "                    DPy.append(float(y_coordinate))\n",
    "                    Decx.append(int(xest_coordinate))\n",
    "                    Decy.append(int(yest_coordinate))\n",
    "\n",
    "                coords = {'orgcoord_x': Dx,\n",
    "                        'orgcoord_y': Dy,\n",
    "                        'percoord_x': DPx,\n",
    "                        'percoord_y': DPy,  \n",
    "                        'decoord_x': Decx,\n",
    "                        'decoord_y': Decy\n",
    "                        }\n",
    "                Sdata = pd.DataFrame(coords)\n",
    "                Sdata.to_csv(svg_path)\n",
    "\n",
    "            print(i, my_strategy,\"----------- Well Done----------\")\n",
    "    if x == simDa[1]:\n",
    "        # Even partition\n",
    "        dataset_numbers = [4, 5, 6, 7, 8]\n",
    "        fname = re.split(r'/', new_file_path)[-1]\n",
    "        print(fname)\n",
    "    \n",
    "        for dataset_number in dataset_numbers: \n",
    "            even_split_file = f'{new_file_path}/{x}/{dataset_number}/'\n",
    "            os.listdir(even_split_file)\n",
    "            strategies = ['is_shuffled_True', 'is_shuffled_False']\n",
    "            for my_strategy in strategies: \n",
    "                file_pattern = f\"*[0-9]_{my_strategy}.csv\"\n",
    "\n",
    "                # 计算文件数量\n",
    "                dataset_number = count_files_with_numeric_pattern(even_split_file, file_pattern)\n",
    "\n",
    "                for j in range(dataset_number):\n",
    "                    # sub data set\n",
    "                    origfpath = f'{even_split_file}/even_patchesnames_{str(j)}_{my_strategy}.csv'\n",
    "                    encodefpath = f'{even_split_file}{my_strategy}_R_encode_coordinates_{j}.csv'\n",
    "                    decodefpath = f'{even_split_file}{my_strategy}_R_decode_coordinates_{j}.csv'\n",
    "                    svg_path = f'{coordfp}/{x}/{dataset_number}/{i}{fname}_coords_{my_strategy}_{j}.csv'\n",
    "        \n",
    "                    perturbe_df = pd.read_csv(encodefpath)\n",
    "                    orig_df = pd.read_csv(origfpath)\n",
    "                    estimate_df = pd.read_csv(decodefpath)\n",
    "\n",
    "                    PerturbedList = perturbe_df[\"EncryPatchName\"].tolist()\n",
    "                    origList = orig_df[\"PatchName\"].tolist()\n",
    "                    estList = estimate_df[\"DecryPatchName\"].tolist()\n",
    "                    print(len(PerturbedList), len(origList), len(estList))\n",
    "\n",
    "                    Dx = []\n",
    "                    Dy = []\n",
    "                    DPx = []\n",
    "                    DPy = []\n",
    "                    Decx = []\n",
    "                    Decy = []\n",
    "                    for d, dp, dec in zip(origList, PerturbedList, estList): \n",
    "                        # print(i,c,folder)\n",
    "                        (xorg_coordinate, yorg_coordinate) = find_coordinates(d) # type: ignore\n",
    "                        (x_coordinate, y_coordinate) = find_encrpytcoordinates(dp) # type: ignore\n",
    "                        (xest_coordinate, yest_coordinate) = find_coordinates(dec) # type: ignore\n",
    "                        Dx.append(int(xorg_coordinate))\n",
    "                        Dy.append(int(yorg_coordinate))\n",
    "                        DPx.append(float(x_coordinate))\n",
    "                        DPy.append(float(y_coordinate))\n",
    "                        Decx.append(int(xest_coordinate))\n",
    "                        Decy.append(int(yest_coordinate))\n",
    "\n",
    "                    coords = {'orgcoord_x': Dx,\n",
    "                            'orgcoord_y': Dy,\n",
    "                            'percoord_x': DPx,\n",
    "                            'percoord_y': DPy,  \n",
    "                            'decoord_x': Decx,\n",
    "                            'decoord_y': Decy\n",
    "                            }\n",
    "                    Sdata = pd.DataFrame(coords)\n",
    "                    Sdata.to_csv(svg_path)\n",
    "\n",
    "                print(i, my_strategy,\"----------- Well Done----------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Privacy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import statistics\n",
    "from entropies import entropy1\n",
    "\n",
    "def adjacent_values(vals, q1, q3):\n",
    "    upper_adjacent_value = q3 + (q3 - q1) * 1.5\n",
    "    upper_adjacent_value = np.clip(upper_adjacent_value, q3, vals[-1])\n",
    "\n",
    "    lower_adjacent_value = q1 - (q3 - q1) * 1.5\n",
    "    lower_adjacent_value = np.clip(lower_adjacent_value, vals[0], q1)\n",
    "    return lower_adjacent_value, upper_adjacent_value\n",
    "\n",
    "\n",
    "def set_axis_style(ax, labels):\n",
    "    ax.set_xticks(np.arange(1, len(labels) + 1), labels=labels)\n",
    "    ax.set_xlim(0.25, len(labels) + 0.75)\n",
    "    # ax.set_xlabel('Sample name')\n",
    "\n",
    "# 补齐数列\n",
    "def bq(X,Y): \n",
    "    # Assuming len(y) < len(X)\n",
    "    if len(X) < len(Y):\n",
    "        reX = X + [0] * (abs(len(X) - len(Y)))\n",
    "        X = reX\n",
    "    elif len(X) > len(Y):\n",
    "        reY = Y + [0] * (abs(len(X) - len(Y)))\n",
    "        Y = reY    \n",
    "    return X, Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMI with original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simDa = ['graph_split1', 'even_split']\n",
    "x = simDa[0]\n",
    "\n",
    "fid = str(len(new_file_paths)-1)\n",
    "\n",
    "privacy4strategies = {}    \n",
    "if x == simDa[0]:\n",
    "    coordfp = \"/root/code2024/pipeline/YD/privacy_eval/coordinates/graph_split1/\"\n",
    "    strategies = ['largest_first', 'random_sequential', 'smallest_last', 'independent_set', 'connected_sequential', 'saturation_largest_first']\n",
    "\n",
    "    print(fname)\n",
    "    \n",
    "    for my_strategy in strategies: \n",
    "        totfpath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}.csv')\n",
    "        _split_file = f'{coordfp}'\n",
    "        # _split_file = f'{new_file_path}/{x}/'\n",
    "        # 设置你的目标文件夹名称和文件模式\n",
    "        file_pattern = f\"*{fname}_coords_{my_strategy}_[0-9].csv\"\n",
    "        dataset_number = count_files_with_numeric_pattern(_split_file, file_pattern)\n",
    "        subfno = [j for j in range(dataset_number)]\n",
    "        \n",
    "        all_dataX_per_s = []\n",
    "        all_dataY_per_s = []\n",
    "\n",
    "        IGx = []\n",
    "        IGy = []\n",
    "\n",
    "        NIx = []\n",
    "        NIy = []\n",
    "        \n",
    "        for new_file_path in new_file_paths: \n",
    "            Dx = []\n",
    "            Dy = []\n",
    "            Dpx= []\n",
    "            Dpy = []\n",
    "            Dex= []\n",
    "            Dey = []\n",
    "            \n",
    "            NMIX = []\n",
    "            NMIY = []\n",
    "            fname = re.split(r'/', new_file_path)[-1]\n",
    "            for j in range(len(subfno)):\n",
    "                ifilepath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{j}.csv')\n",
    "                \n",
    "                # print(coordfp, ifilepath)\n",
    "                idata = pd.read_csv(ifilepath)\n",
    "            \n",
    "                # print(idata)\n",
    "                subfno.remove(j)\n",
    "                rst = subfno\n",
    "                print(fid, j, rst)\n",
    "                cfilepath = [os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{k}.csv') for k in rst]\n",
    "                \n",
    "                \n",
    "                rstdata = [pd.read_csv(cfilepath[x]) for x in range(len(rst))]\n",
    "                cdata = pd.concat(rstdata)\n",
    "\n",
    "                X = idata['orgcoord_x'].tolist()\n",
    "                Y = cdata['orgcoord_x'].tolist()\n",
    "                ecodX = idata['percoord_x'].tolist()\n",
    "                decodX = idata['decoord_x'].tolist()\n",
    "\n",
    "                Z = idata['orgcoord_y'].tolist()\n",
    "                W = cdata['orgcoord_y'].tolist()\n",
    "                ecodZ = idata['percoord_y'].tolist()\n",
    "                decodZ = idata['decoord_y'].tolist()\n",
    "\n",
    "                # print(\"X\", len(X), len(Y), len(Z), len(W))\n",
    "                Dx.extend(X)\n",
    "                Dy.extend(Z)\n",
    "                \n",
    "                Dpx.extend(ecodX)\n",
    "                Dpy.extend(ecodZ)\n",
    "                Dex.extend(decodX)\n",
    "                Dey.extend(decodZ)\n",
    "                \n",
    "                \n",
    "                # Calculate mutual information\n",
    "                from scipy.stats.contingency import crosstab\n",
    "                from sklearn.metrics import mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "                # output utility\n",
    "                valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                valuex,counts1dex = np.unique(decodX, return_inverse=True)\n",
    "                \n",
    "                valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                valuex,counts1dey = np.unique(decodZ, return_inverse=True)\n",
    "            \n",
    "                nmix_dex = normalized_mutual_info_score(counts1x, counts1dex)\n",
    "                nmiy_dey = normalized_mutual_info_score(counts1y, counts1dey)\n",
    "                # print(my_strategy, \"individual output utility: \", nmix_dex, nmiy_dey)\n",
    "\n",
    "                \n",
    "                # individual privacy lower bound\n",
    "                X, Y = bq(X,Y)\n",
    "                Z, W = bq(Z,W)\n",
    "            \n",
    "                valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                value2x,counts2x = np.unique(Y, return_inverse=True)\n",
    "\n",
    "                valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                value2x,counts2y = np.unique(W, return_inverse=True)\n",
    "\n",
    "                nmix = normalized_mutual_info_score(counts1x, counts2x)\n",
    "                nmiy = normalized_mutual_info_score(counts1y, counts2y)\n",
    "            \n",
    "                rst.append(j)\n",
    "\n",
    "                NMIX.append(nmix)\n",
    "                NMIY.append(nmiy)\n",
    "\n",
    "                # print(rst, subfno)\n",
    "                # print(my_strategy, \"individual privacy at lower bound: \", nmix, nmiy)\n",
    "                # print('--------------')\n",
    "            \n",
    "            all_dataX_per_s.append(NMIX)\n",
    "            all_dataY_per_s.append(NMIY)\n",
    "\n",
    "            # print(f\"NMI_{my_strategy}\", NMIX, NMIY)\n",
    "            # print(f\"NMI_stat_{my_strategy}\", statistics.mean(NMIX), statistics.stdev(NMIX), statistics.mean(NMIY), statistics.stdev(NMIY),'\\n')\n",
    "\n",
    "            # Calculate AIG\n",
    "            x = entropy1(Dpx)-entropy1(Dx)\n",
    "            y = entropy1(Dpy)-entropy1(Dy)\n",
    "            IGx.append(x)\n",
    "            IGy.append(y)\n",
    "        \n",
    "            # print(f\"IG_{my_strategy}\", IGx, IGy)\n",
    "            # Calculate NI:lower bound on individual privacy\n",
    "            # print(my_strategy, len(Dpx), len(Dx), len(Dpy), len(Dy))\n",
    "            diffx_NI = [np.abs(Dpx[x] - Dx[x]) for x in range(len(Dx))] \n",
    "            diffy_NI = [np.abs(Dpy[x] - Dy[x]) for x in range(len(Dy))]     \n",
    "\n",
    "            NIx.extend(diffx_NI)\n",
    "            NIy.extend(diffy_NI)\n",
    "        \n",
    "        evalx_NI_std = np.std(NIx)\n",
    "        evaly_NI_std = np.std(NIy) \n",
    "        evalx_NI_mean = np.mean(NIx)\n",
    "        evaly_NI_mean = np.mean(NIy) \n",
    "        \n",
    "        print(my_strategy, \"NI(all data)\", f'{evalx_NI_mean}+/-{round(evalx_NI_std, 8)}, {evaly_NI_mean}+/-{round(evaly_NI_std, 8)}')\n",
    "\n",
    "        AIGx = round(sum(IGx)/len(IGx),8)\n",
    "        AIGy = round(sum(IGy)/len(IGy),8)\n",
    "        print(my_strategy, \"AIG(all data)\", round(AIGx, 4), round(AIGy, 4))\n",
    "\n",
    "        eval_NMIX_mean = np.mean(all_dataX_per_s)\n",
    "        eval_NMIY_mean = np.mean(all_dataY_per_s)\n",
    "        eval_NMIX_std = np.std(all_dataX_per_s)\n",
    "        eval_NMIY_std = np.std(all_dataY_per_s)\n",
    "        \n",
    "        privacy4strategies[my_strategy] = (round(eval_NMIX_mean, 3), round(eval_NMIY_mean,3))\n",
    "        print(my_strategy, \"NMI--individual privacy at lower bound(all data)\", f'{round(eval_NMIX_mean, 3)}+/-{round(eval_NMIX_std, 3)}, {round(eval_NMIY_mean,3)}+/-{round(eval_NMIY_std, 3)}')\n",
    "    print(privacy4strategies)\n",
    "\n",
    "elif x == simDa[1]:\n",
    "    dataset_numbers = [4,5,6,7,8]\n",
    "    for dataset_number in dataset_numbers: \n",
    "        print(fname)\n",
    "        coordfp = f\"/root/code2024/pipeline/YD/privacy_eval/coordinates/even_split/{dataset_number}/\"\n",
    "        # print(coordfp)\n",
    "        # strategies = ['is_shuffled_True', 'is_shuffled_False']\n",
    "        # strategies = ['is_shuffled_False']\n",
    "        strategies = ['is_shuffled_True']\n",
    "        for my_strategy in strategies: \n",
    "            totfpath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}.csv')\n",
    "            _split_file = f'{coordfp}'\n",
    "            # 设置你的目标文件夹名称和文件模式\n",
    "            file_pattern = f\"*{fname}_coords_{my_strategy}_[0-9].csv\"\n",
    "            dataset_number = count_files_with_numeric_pattern(_split_file, file_pattern)\n",
    "            subfno = [j for j in range(dataset_number)]\n",
    "            \n",
    "            all_dataX_per_s = []\n",
    "            all_dataY_per_s = []\n",
    "\n",
    "            IGx = []\n",
    "            IGy = []\n",
    "\n",
    "            NIx = []\n",
    "            NIy = []\n",
    "            \n",
    "            for new_file_path in new_file_paths: \n",
    "                Dx = []\n",
    "                Dy = []\n",
    "                Dpx= []\n",
    "                Dpy = []\n",
    "                Dex= []\n",
    "                Dey = []\n",
    "                \n",
    "                NMIX = []\n",
    "                NMIY = []\n",
    "                fname = re.split(r'/', new_file_path)[-1]\n",
    "                for j in range(len(subfno)):\n",
    "                    ifilepath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{j}.csv')\n",
    "                    \n",
    "                    # print(coordfp, ifilepath)\n",
    "                    idata = pd.read_csv(ifilepath)\n",
    "                \n",
    "                    # print(idata)\n",
    "                    subfno.remove(j)\n",
    "                    rst = subfno\n",
    "                    print(fid, j, rst)\n",
    "                    cfilepath = [os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{k}.csv') for k in rst]\n",
    "                    \n",
    "                    \n",
    "                    rstdata = [pd.read_csv(cfilepath[x]) for x in range(len(rst))]\n",
    "                    cdata = pd.concat(rstdata)\n",
    "\n",
    "                    X = idata['orgcoord_x'].tolist()\n",
    "                    Y = cdata['orgcoord_x'].tolist()\n",
    "                    ecodX = idata['percoord_x'].tolist()\n",
    "                    decodX = idata['decoord_x'].tolist()\n",
    "\n",
    "                    Z = idata['orgcoord_y'].tolist()\n",
    "                    W = cdata['orgcoord_y'].tolist()\n",
    "                    ecodZ = idata['percoord_y'].tolist()\n",
    "                    decodZ = idata['decoord_y'].tolist()\n",
    "\n",
    "                    # print(\"X\", len(X), len(Y), len(Z), len(W))\n",
    "                    Dx.extend(X)\n",
    "                    Dy.extend(Z)\n",
    "                    \n",
    "                    Dpx.extend(ecodX)\n",
    "                    Dpy.extend(ecodZ)\n",
    "                    Dex.extend(decodX)\n",
    "                    Dey.extend(decodZ)\n",
    "                    \n",
    "                    \n",
    "                    # Calculate mutual information\n",
    "                    from scipy.stats.contingency import crosstab\n",
    "                    from sklearn.metrics import mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "                    # output utility\n",
    "                    valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                    valuex,counts1dex = np.unique(decodX, return_inverse=True)\n",
    "                    \n",
    "                    valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                    valuex,counts1dey = np.unique(decodZ, return_inverse=True)\n",
    "                \n",
    "                    nmix_dex = normalized_mutual_info_score(counts1x, counts1dex)\n",
    "                    nmiy_dey = normalized_mutual_info_score(counts1y, counts1dey)\n",
    "                    # print(my_strategy, \"individual output utility: \", nmix_dex, nmiy_dey)\n",
    "\n",
    "                    \n",
    "                    # individual privacy lower bound\n",
    "                    X, Y = bq(X,Y)\n",
    "                    Z, W = bq(Z,W)\n",
    "                \n",
    "                    valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                    value2x,counts2x = np.unique(Y, return_inverse=True)\n",
    "\n",
    "                    valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                    value2x,counts2y = np.unique(W, return_inverse=True)\n",
    "\n",
    "                    nmix = normalized_mutual_info_score(counts1x, counts2x)\n",
    "                    nmiy = normalized_mutual_info_score(counts1y, counts2y)\n",
    "                \n",
    "                    rst.append(j)\n",
    "\n",
    "                    NMIX.append(nmix)\n",
    "                    NMIY.append(nmiy)\n",
    "\n",
    "                    # print(rst, subfno)\n",
    "                    # print(my_strategy, \"individual privacy at lower bound: \", nmix, nmiy)\n",
    "                    # print('--------------')\n",
    "                \n",
    "                all_dataX_per_s.append(NMIX)\n",
    "                all_dataY_per_s.append(NMIY)\n",
    "\n",
    "                # print(f\"NMI_{my_strategy}\", NMIX, NMIY)\n",
    "                # print(f\"NMI_statis_{my_strategy}\",\"individual privacy at lower bound: \", statistics.mean(NMIX), statistics.stdev(NMIX), statistics.mean(NMIY), statistics.stdev(NMIY),'\\n')\n",
    "\n",
    "                # Calculate AIG\n",
    "                x = entropy1(Dpx)-entropy1(Dx)\n",
    "                y = entropy1(Dpy)-entropy1(Dy)\n",
    "                IGx.append(x)\n",
    "                IGy.append(y)\n",
    "            \n",
    "                # print(f\"IG_{my_strategy}\", IGx, IGy)\n",
    "\n",
    "                # Calculate NI:lower bound on individual privacy\n",
    "                # print(my_strategy, len(Dpx), len(Dx), len(Dpy), len(Dy))\n",
    "                diffx_NI = [np.abs(Dpx[x] - Dx[x]) for x in range(len(Dx))] \n",
    "                diffy_NI = [np.abs(Dpy[x] - Dy[x]) for x in range(len(Dy))]     \n",
    "\n",
    "                NIx.extend(diffx_NI)\n",
    "                NIy.extend(diffy_NI)\n",
    "            \n",
    "            evalx_NI_std = np.std(NIx)\n",
    "            evaly_NI_std = np.std(NIy) \n",
    "            evalx_NI_mean = np.mean(NIx)\n",
    "            evaly_NI_mean = np.mean(NIy) \n",
    "            \n",
    "            # print(my_strategy, \"NI(all data)\", f'{evalx_NI_mean}+/-{round(evalx_NI_std, 8)}, {evaly_NI_mean}+/-{round(evaly_NI_std, 8)}')\n",
    "\n",
    "            AIGx = round(sum(IGx)/len(IGx),8)\n",
    "            AIGy = round(sum(IGy)/len(IGy),8)\n",
    "            # print(my_strategy, \"AIG(all data)\", round(AIGx, 4), round(AIGy, 4))\n",
    "\n",
    "            eval_NMIX_mean = np.mean(all_dataX_per_s)\n",
    "            eval_NMIY_mean = np.mean(all_dataY_per_s)\n",
    "            eval_NMIX_std = np.std(all_dataX_per_s)\n",
    "            eval_NMIY_std = np.std(all_dataY_per_s)\n",
    "\n",
    "            print(my_strategy, \"NMI--individual privacy at lower bound(all data)\", f'{round(eval_NMIX_mean, 3)}+/-{round(eval_NMIX_std, 3)}, {round(eval_NMIY_mean,3)}+/-{round(eval_NMIY_std, 3)}')\n",
    "            privacy4strategies[(my_strategy, dataset_number)] = (round(eval_NMIX_mean, 3), round(eval_NMIY_mean,3))\n",
    "            # print(my_strategy, \"NMI--individual privacy at lower bound(all data)\", f'{round(eval_NMIX_mean, 4)}+-{round(eval_NMIX_std, 4)}, {round(eval_NMIY_mean,4)}+-{round(eval_NMIY_std, 4)}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NMI with encoded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simDa = ['graph_split1', 'even_split']\n",
    "x = simDa[0]\n",
    "\n",
    "fid = str(len(new_file_paths)-1)\n",
    "\n",
    "privacy4strategies = {}    \n",
    "if x == simDa[0]:\n",
    "    coordfp = \"/root/code2024/pipeline/YD/privacy_eval/coordinates/graph_split1/\"\n",
    "    strategies = ['largest_first', 'random_sequential', 'smallest_last', 'independent_set', 'connected_sequential', 'saturation_largest_first']\n",
    "\n",
    "    print(fname)\n",
    "    \n",
    "    for my_strategy in strategies: \n",
    "        totfpath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}.csv')\n",
    "        _split_file = f'{coordfp}'\n",
    "        # _split_file = f'{new_file_path}/{x}/'\n",
    "        # 设置你的目标文件夹名称和文件模式\n",
    "        file_pattern = f\"*{fname}_coords_{my_strategy}_[0-9].csv\"\n",
    "        dataset_number = count_files_with_numeric_pattern(_split_file, file_pattern)\n",
    "        subfno = [j for j in range(dataset_number)]\n",
    "        \n",
    "        all_dataX_per_s = []\n",
    "        all_dataY_per_s = []\n",
    "\n",
    "        IGx = []\n",
    "        IGy = []\n",
    "\n",
    "        NIx = []\n",
    "        NIy = []\n",
    "        \n",
    "        for new_file_path in new_file_paths: \n",
    "            Dx = []\n",
    "            Dy = []\n",
    "            Dpx= []\n",
    "            Dpy = []\n",
    "            Dex= []\n",
    "            Dey = []\n",
    "            \n",
    "            NMIX = []\n",
    "            NMIY = []\n",
    "            fname = re.split(r'/', new_file_path)[-1]\n",
    "            for j in range(len(subfno)):\n",
    "                ifilepath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{j}.csv')\n",
    "                \n",
    "                # print(coordfp, ifilepath)\n",
    "                idata = pd.read_csv(ifilepath)\n",
    "            \n",
    "                # print(idata)\n",
    "                subfno.remove(j)\n",
    "                rst = subfno\n",
    "                print(fid, j, rst)\n",
    "                cfilepath = [os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{k}.csv') for k in rst]\n",
    "                \n",
    "                \n",
    "                rstdata = [pd.read_csv(cfilepath[x]) for x in range(len(rst))]\n",
    "                cdata = pd.concat(rstdata)\n",
    "\n",
    "                X = idata['orgcoord_x'].tolist()\n",
    "                Y = cdata['orgcoord_x'].tolist()\n",
    "                ecodX = idata['percoord_x'].tolist()\n",
    "                ecodY = cdata['percoord_x'].tolist()\n",
    "\n",
    "                decodX = idata['decoord_x'].tolist()\n",
    "                decodY = cdata['decoord_x'].tolist()\n",
    "\n",
    "                Z = idata['orgcoord_y'].tolist()\n",
    "                W = cdata['orgcoord_y'].tolist()\n",
    "                ecodZ = idata['percoord_y'].tolist()\n",
    "                ecodW = cdata['percoord_y'].tolist()\n",
    "\n",
    "                decodZ = idata['decoord_y'].tolist()\n",
    "                decodW = cdata['decoord_y'].tolist()\n",
    "\n",
    "                # print(\"X\", len(X), len(Y), len(Z), len(W))\n",
    "                Dx.extend(X)\n",
    "                Dy.extend(Z)\n",
    "                \n",
    "                Dpx.extend(ecodX)\n",
    "                Dpy.extend(ecodZ)\n",
    "                Dex.extend(decodX)\n",
    "                Dey.extend(decodZ)\n",
    "                \n",
    "                \n",
    "                # Calculate mutual information\n",
    "                from scipy.stats.contingency import crosstab\n",
    "                from sklearn.metrics import mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "                # output utility\n",
    "                valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                valuex,counts1dex = np.unique(decodX, return_inverse=True)\n",
    "                \n",
    "                valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                valuex,counts1dey = np.unique(decodZ, return_inverse=True)\n",
    "            \n",
    "                nmix_dex = normalized_mutual_info_score(counts1x, counts1dex)\n",
    "                nmiy_dey = normalized_mutual_info_score(counts1y, counts1dey)\n",
    "                print(my_strategy, \"individual output utility: \", nmix_dex, nmiy_dey)\n",
    "\n",
    "                \n",
    "                # individual privacy lower bound\n",
    "                # X, Y = bq(X,Y)\n",
    "                # Z, W = bq(Z,W)\n",
    "            \n",
    "                # valuex,counts1x = np.unique(X, return_inverse=True)     # original x(i)\n",
    "                # value2x,counts2x = np.unique(Y, return_inverse=True)    # original x(j)\n",
    "\n",
    "                # valuex,counts1y = np.unique(Z, return_inverse=True)     # original y(i)\n",
    "                # value2x,counts2y = np.unique(W, return_inverse=True)    # # original y(j)\n",
    "\n",
    "                # nmix = normalized_mutual_info_score(counts1x, counts2x)\n",
    "                # nmiy = normalized_mutual_info_score(counts1y, counts2y)\n",
    "\n",
    "                ecodX, ecodY = bq(X,Y)\n",
    "                ecodZ, ecodW = bq(Z,W)\n",
    "            \n",
    "\n",
    "                valueedx,counts1edx = np.unique(ecodX, return_inverse=True)     # encode x(i)\n",
    "                valueed2x,counts2edx = np.unique(ecodY, return_inverse=True)    # encode x(j)\n",
    "\n",
    "                valueedx,counts1edy = np.unique(ecodZ, return_inverse=True)     # encode y(i)\n",
    "                valueed2x,counts2edy = np.unique(ecodW, return_inverse=True)    # # encode y(j)\n",
    "\n",
    "                nmix = normalized_mutual_info_score(counts1edx, counts2edx)\n",
    "                nmiy = normalized_mutual_info_score(counts1edy, counts2edy)\n",
    "            \n",
    "                rst.append(j)\n",
    "\n",
    "                NMIX.append(nmix)\n",
    "                NMIY.append(nmiy)\n",
    "\n",
    "                # print(rst, subfno)\n",
    "                # print(my_strategy, \"individual privacy at lower bound: \", nmix, nmiy)\n",
    "                # print('--------------')\n",
    "            \n",
    "            all_dataX_per_s.append(NMIX)\n",
    "            all_dataY_per_s.append(NMIY)\n",
    "\n",
    "            # print(f\"NMI_{my_strategy}\", NMIX, NMIY)\n",
    "            # print(f\"NMI_stat_{my_strategy}\", statistics.mean(NMIX), statistics.stdev(NMIX), statistics.mean(NMIY), statistics.stdev(NMIY),'\\n')\n",
    "\n",
    "            # Calculate AIG\n",
    "            x = entropy1(Dpx)-entropy1(Dx)\n",
    "            y = entropy1(Dpy)-entropy1(Dy)\n",
    "            IGx.append(x)\n",
    "            IGy.append(y)\n",
    "        \n",
    "            # print(f\"IG_{my_strategy}\", IGx, IGy)\n",
    "            # Calculate NI:lower bound on individual privacy\n",
    "            # print(my_strategy, len(Dpx), len(Dx), len(Dpy), len(Dy))\n",
    "            diffx_NI = [np.abs(Dpx[x] - Dx[x]) for x in range(len(Dx))] \n",
    "            diffy_NI = [np.abs(Dpy[x] - Dy[x]) for x in range(len(Dy))]     \n",
    "            # diffx_NI = [np.abs(Dex[x] - Dx[x]) for x in range(len(Dx))] \n",
    "            # diffy_NI = [np.abs(Dey[x] - Dy[x]) for x in range(len(Dy))]     \n",
    "\n",
    "\n",
    "            NIx.extend(diffx_NI)\n",
    "            NIy.extend(diffy_NI)\n",
    "        \n",
    "        evalx_NI_std = np.std(NIx)\n",
    "        evaly_NI_std = np.std(NIy) \n",
    "        evalx_NI_mean = np.mean(NIx)\n",
    "        evaly_NI_mean = np.mean(NIy) \n",
    "        \n",
    "        print(my_strategy, \"NI(all data)\", f'{evalx_NI_mean}+/-{round(evalx_NI_std, 8)}, {evaly_NI_mean}+/-{round(evaly_NI_std, 8)}')\n",
    "\n",
    "        AIGx = round(sum(IGx)/len(IGx),8)\n",
    "        AIGy = round(sum(IGy)/len(IGy),8)\n",
    "        print(my_strategy, \"AIG(all data)\", round(AIGx, 4), round(AIGy, 4))\n",
    "\n",
    "        eval_NMIX_mean = np.mean(all_dataX_per_s)\n",
    "        eval_NMIY_mean = np.mean(all_dataY_per_s)\n",
    "        eval_NMIX_std = np.std(all_dataX_per_s)\n",
    "        eval_NMIY_std = np.std(all_dataY_per_s)\n",
    "        \n",
    "        privacy4strategies[my_strategy] = (round(eval_NMIX_mean, 3), round(eval_NMIY_mean,3))\n",
    "        print(my_strategy, \"NMI--individual privacy at lower bound(all data)\", f'{round(eval_NMIX_mean, 3)}+/-{round(eval_NMIX_std, 3)}, {round(eval_NMIY_mean,3)}+/-{round(eval_NMIY_std, 3)}')\n",
    "    print(privacy4strategies)\n",
    "\n",
    "elif x == simDa[1]:\n",
    "    dataset_numbers = [4,5,6,7,8]\n",
    "    for dataset_number in dataset_numbers: \n",
    "        print(fname)\n",
    "        coordfp = f\"/root/code2024/pipeline/YD/privacy_eval/coordinates/even_split/{dataset_number}/\"\n",
    "        # print(coordfp)\n",
    "        # strategies = ['is_shuffled_True', 'is_shuffled_False']\n",
    "        # strategies = ['is_shuffled_False']\n",
    "        strategies = ['is_shuffled_True']\n",
    "        for my_strategy in strategies: \n",
    "            totfpath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}.csv')\n",
    "            _split_file = f'{coordfp}'\n",
    "            # 设置你的目标文件夹名称和文件模式\n",
    "            file_pattern = f\"*{fname}_coords_{my_strategy}_[0-9].csv\"\n",
    "            dataset_number = count_files_with_numeric_pattern(_split_file, file_pattern)\n",
    "            subfno = [j for j in range(dataset_number)]\n",
    "            \n",
    "            all_dataX_per_s = []\n",
    "            all_dataY_per_s = []\n",
    "\n",
    "            IGx = []\n",
    "            IGy = []\n",
    "\n",
    "            NIx = []\n",
    "            NIy = []\n",
    "            \n",
    "            for new_file_path in new_file_paths: \n",
    "                Dx = []\n",
    "                Dy = []\n",
    "                Dpx= []\n",
    "                Dpy = []\n",
    "                Dex= []\n",
    "                Dey = []\n",
    "                \n",
    "                NMIX = []\n",
    "                NMIY = []\n",
    "                fname = re.split(r'/', new_file_path)[-1]\n",
    "                for j in range(len(subfno)):\n",
    "                    ifilepath = os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{j}.csv')\n",
    "                    \n",
    "                    # print(coordfp, ifilepath)\n",
    "                    idata = pd.read_csv(ifilepath)\n",
    "                \n",
    "                    # print(idata)\n",
    "                    subfno.remove(j)\n",
    "                    rst = subfno\n",
    "                    print(fid, j, rst)\n",
    "                    cfilepath = [os.path.join(coordfp, f'{fid}{fname}_coords_{my_strategy}_{k}.csv') for k in rst]\n",
    "                    \n",
    "                    \n",
    "                    rstdata = [pd.read_csv(cfilepath[x]) for x in range(len(rst))]\n",
    "                    cdata = pd.concat(rstdata)\n",
    "\n",
    "                    X = idata['orgcoord_x'].tolist()\n",
    "                    Y = cdata['orgcoord_x'].tolist()\n",
    "                    ecodX = idata['percoord_x'].tolist()\n",
    "                    decodX = idata['decoord_x'].tolist()\n",
    "\n",
    "                    Z = idata['orgcoord_y'].tolist()\n",
    "                    W = cdata['orgcoord_y'].tolist()\n",
    "                    ecodZ = idata['percoord_y'].tolist()\n",
    "                    decodZ = idata['decoord_y'].tolist()\n",
    "\n",
    "                    # print(\"X\", len(X), len(Y), len(Z), len(W))\n",
    "                    Dx.extend(X)\n",
    "                    Dy.extend(Z)\n",
    "                    \n",
    "                    Dpx.extend(ecodX)\n",
    "                    Dpy.extend(ecodZ)\n",
    "                    Dex.extend(decodX)\n",
    "                    Dey.extend(decodZ)\n",
    "                    \n",
    "                    \n",
    "                    # Calculate mutual information\n",
    "                    from scipy.stats.contingency import crosstab\n",
    "                    from sklearn.metrics import mutual_info_score, normalized_mutual_info_score, adjusted_mutual_info_score\n",
    "\n",
    "                    # output utility\n",
    "                    valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                    valuex,counts1dex = np.unique(decodX, return_inverse=True)\n",
    "                    \n",
    "                    valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                    valuex,counts1dey = np.unique(decodZ, return_inverse=True)\n",
    "                \n",
    "                    nmix_dex = normalized_mutual_info_score(counts1x, counts1dex)\n",
    "                    nmiy_dey = normalized_mutual_info_score(counts1y, counts1dey)\n",
    "                    # print(my_strategy, \"individual output utility: \", nmix_dex, nmiy_dey)\n",
    "\n",
    "                    \n",
    "                    # individual privacy lower bound\n",
    "                    X, Y = bq(X,Y)\n",
    "                    Z, W = bq(Z,W)\n",
    "                \n",
    "                    valuex,counts1x = np.unique(X, return_inverse=True)\n",
    "                    value2x,counts2x = np.unique(Y, return_inverse=True)\n",
    "\n",
    "                    valuex,counts1y = np.unique(Z, return_inverse=True)\n",
    "                    value2x,counts2y = np.unique(W, return_inverse=True)\n",
    "\n",
    "                    nmix = normalized_mutual_info_score(counts1x, counts2x)\n",
    "                    nmiy = normalized_mutual_info_score(counts1y, counts2y)\n",
    "                \n",
    "                    rst.append(j)\n",
    "\n",
    "                    NMIX.append(nmix)\n",
    "                    NMIY.append(nmiy)\n",
    "\n",
    "                    # print(rst, subfno)\n",
    "                    # print(my_strategy, \"individual privacy at lower bound: \", nmix, nmiy)\n",
    "                    # print('--------------')\n",
    "                \n",
    "                all_dataX_per_s.append(NMIX)\n",
    "                all_dataY_per_s.append(NMIY)\n",
    "\n",
    "                # print(f\"NMI_{my_strategy}\", NMIX, NMIY)\n",
    "                # print(f\"NMI_statis_{my_strategy}\",\"individual privacy at lower bound: \", statistics.mean(NMIX), statistics.stdev(NMIX), statistics.mean(NMIY), statistics.stdev(NMIY),'\\n')\n",
    "\n",
    "                # Calculate AIG\n",
    "                x = entropy1(Dpx)-entropy1(Dx)\n",
    "                y = entropy1(Dpy)-entropy1(Dy)\n",
    "                IGx.append(x)\n",
    "                IGy.append(y)\n",
    "            \n",
    "                # print(f\"IG_{my_strategy}\", IGx, IGy)\n",
    "\n",
    "                # Calculate NI:lower bound on individual privacy\n",
    "                # print(my_strategy, len(Dpx), len(Dx), len(Dpy), len(Dy))\n",
    "                # diffx_NI = [np.abs(Dpx[x] - Dx[x]) for x in range(len(Dx))] \n",
    "                # diffy_NI = [np.abs(Dpy[x] - Dy[x]) for x in range(len(Dy))]     \n",
    "\n",
    "                diffx_NI = [np.abs(Dx[x] - Dx[x]) for x in range(len(Dx))] \n",
    "                diffy_NI = [np.abs(Dy[x] - Dy[x]) for x in range(len(Dy))]     \n",
    "\n",
    "                NIx.extend(diffx_NI)\n",
    "                NIy.extend(diffy_NI)\n",
    "            \n",
    "            evalx_NI_std = np.std(NIx)\n",
    "            evaly_NI_std = np.std(NIy) \n",
    "            evalx_NI_mean = np.mean(NIx)\n",
    "            evaly_NI_mean = np.mean(NIy) \n",
    "            \n",
    "            # print(my_strategy, \"NI(all data)\", f'{evalx_NI_mean}+/-{round(evalx_NI_std, 8)}, {evaly_NI_mean}+/-{round(evaly_NI_std, 8)}')\n",
    "\n",
    "            AIGx = round(sum(IGx)/len(IGx),8)\n",
    "            AIGy = round(sum(IGy)/len(IGy),8)\n",
    "            # print(my_strategy, \"AIG(all data)\", round(AIGx, 4), round(AIGy, 4))\n",
    "\n",
    "            eval_NMIX_mean = np.mean(all_dataX_per_s)\n",
    "            eval_NMIY_mean = np.mean(all_dataY_per_s)\n",
    "            eval_NMIX_std = np.std(all_dataX_per_s)\n",
    "            eval_NMIY_std = np.std(all_dataY_per_s)\n",
    "\n",
    "            print(my_strategy, \"NMI--individual privacy at lower bound(all data)\", f'{round(eval_NMIX_mean, 3)}+/-{round(eval_NMIX_std, 3)}, {round(eval_NMIY_mean,3)}+/-{round(eval_NMIY_std, 3)}')\n",
    "            privacy4strategies[(my_strategy, dataset_number)] = (round(eval_NMIX_mean, 3), round(eval_NMIY_mean,3))\n",
    "            # print(my_strategy, \"NMI--individual privacy at lower bound(all data)\", f'{round(eval_NMIX_mean, 4)}+-{round(eval_NMIX_std, 4)}, {round(eval_NMIY_mean,4)}+-{round(eval_NMIY_std, 4)}')\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = []\n",
    "startegies = []\n",
    "x_privacy = []\n",
    "y_privacy = []\n",
    "label = 'is_shuffled_True'\n",
    "print(privacy4strategies)\n",
    "for i in privacy4strategies.keys():\n",
    "    if i[0] == label:\n",
    "        print(i, privacy4strategies[i])\n",
    "        N.append(i[1])\n",
    "        startegies.append(i)\n",
    "        x_privacy.append(privacy4strategies[i][0])\n",
    "        y_privacy.append(privacy4strategies[i][1])\n",
    "\n",
    "\n",
    "# 相同的份数，不同的策略"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 双坐标轴图 -- 统计分析individual privacy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
