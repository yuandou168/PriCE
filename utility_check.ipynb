{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Utility Evaluation\n",
    "\n",
    "## using the decrypted coordinate information to restor the entire gigapixel image and compare it with the original one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the gigapixel image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Ignore all FutureWarning warnings that might flood the console log\n",
    "warnings.simplefilter(action='ignore', category=DeprecationWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "\n",
    "# from encryption import encode, decode\n",
    "\n",
    "import time\n",
    "from utils import create_patches_v3, create_binary_mask, create_patches_v2, create_patches_v1, create_patches_coordinates, create_patches_coordinates_v1\n",
    "\n",
    "# Loading directory\n",
    "wsi_dir = \"/root/code2024/dataset/1WSI/data/\"\n",
    "patch_size = 224\n",
    "cpu_core = 12\n",
    "\n",
    "# Saving directory\n",
    "save_dir = wsi_dir\n",
    "\n",
    "downsize = patch_size\n",
    "patch_extraction_size = patch_size\n",
    "mask_overlap = 80.0\n",
    "batch_size = 64\n",
    "cpu_workers = cpu_core\n",
    "use_prob_threshold = 0.8 # None  # whether to give final prediction {0,1} based on certain probability\n",
    "\n",
    "# torch.manual_seed(250)\n",
    "print('--------------Start--------------')\n",
    "# read the files\n",
    "wsi_files = os.listdir(wsi_dir)\n",
    "# get all files except temp directory containing patches\n",
    "wsi_files = [f for f in wsi_files if f.endswith(\"svs\") or f.endswith(\"ndpi\") or f.endswith(\"mrxs\")]\n",
    "\n",
    "print(f\"Total files in {wsi_dir} directory are {len(wsi_files)}\")\n",
    "\n",
    "path = os.path.join(wsi_dir, \"cnn_ensemble_updated\")\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "\n",
    "lst_patching_time = []\n",
    "\n",
    "stat_NMI_graph = [] \n",
    "stat_NMI_base = []\n",
    "coords = []\n",
    "new_file_paths = []\n",
    "# start patching process over a certain WSI\n",
    "for i, f in enumerate(wsi_files):\n",
    "\n",
    "    st_binarymask = time.time()\n",
    "    new_file_path = os.path.join(path, f.split(\".\")[0])\n",
    "    print(new_file_path)\n",
    "\n",
    "    # just take the name not extension\n",
    "    if not os.path.exists(new_file_path):\n",
    "        os.mkdir(new_file_path)\n",
    "        \n",
    "    # find binary mask to locate tissue on WSI            \n",
    "    w, h = create_binary_mask(wsi_dir, f, new_file_path, downsize=patch_size)\n",
    "\n",
    "    # print(w/224, h/224)\n",
    "    et_binarymask = time.time()\n",
    "\n",
    "    binarymask_processtime = et_binarymask - st_binarymask\n",
    "    print(f\"Time spent on create_binary_mask is {binarymask_processtime} for {f}\")\n",
    "        \n",
    "    st_patching = time.time()\n",
    "    patch_folder = os.path.join(new_file_path, \"patches\")\n",
    "    \n",
    "    if not os.path.exists(patch_folder):\n",
    "    # # else:\n",
    "        os.mkdir(patch_folder) \n",
    "    if os.path.exists(patch_folder):\n",
    "    #     os.rmdir(patch_folder)\n",
    "    # elif not os.path.exists(patch_folder):\n",
    "    # # else:\n",
    "    #     os.mkdir(patch_folder) \n",
    "        # coordinates, id_to_coordinates = create_patches_coordinates(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #单进程\n",
    "        # coordinates, id_to_coordinates, patches = create_patches_v1(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #单进程\n",
    "        \n",
    "        # create_patches_v3(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "                    #    patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        \n",
    "        # 1. create coordinates with multi-processing\n",
    "        res = create_patches_coordinates_v1(wsi_dir, f, new_file_path, workers=cpu_workers, patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        \n",
    "        # patches = create_patches_v2(wsi_dir, f, new_file_path, patch_folder, workers=cpu_workers,\n",
    "            # patch_size=patch_extraction_size, mask_overlap=mask_overlap) #多进程\n",
    "        # print(len(coordinates))\n",
    "        # print(len(patches))\n",
    "        # print(id_to_coordinates)\n",
    "\n",
    "        # print(\"Coordinates Done!\", len(res), res[:3])\n",
    "        # coordinates = list(res.values())\n",
    "        coordinates = res.values()\n",
    "        ids = res.keys()\n",
    "        # print(\"Coordinates Done!\", len(res), len(coordinates), coordinates[:3])\n",
    "        coords.append(coordinates)\n",
    "        new_file_paths.append(new_file_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### function of count_files_with_numeric_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fnmatch\n",
    "\n",
    "def count_files_with_numeric_pattern(start_path, file_pattern):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(start_path):\n",
    "        for file in files:\n",
    "            if fnmatch.fnmatch(file, file_pattern):\n",
    "                count += 1\n",
    "                # print(os.path.join(root, file))  # 打印匹配的文件路径\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the decrypted labels to reconstract the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_coordinates import find_decrpytcoordinates\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "de_coords = []    \n",
    "for new_file_path in new_file_paths: \n",
    "    graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "    os.listdir(graph_split_file)\n",
    "    # strategies = ['largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "            # 'connected_sequential', 'saturation_largest_first']\n",
    "    strategies = ['saturation_largest_first']        \n",
    "    for my_strategy in strategies: \n",
    "        \n",
    "        # 设置你的目标文件夹名称和文件模式\n",
    "        graph_split_file = f'{new_file_path}/graph_split1/'\n",
    "        file_pattern = f\"*[0-9]{my_strategy}.csv\"\n",
    "\n",
    "        # 计算文件数量\n",
    "        dataset_number = count_files_with_numeric_pattern(graph_split_file, file_pattern)\n",
    "        print(my_strategy, dataset_number)\n",
    "        \n",
    "        coordinates = []\n",
    "        for j in range(dataset_number):\n",
    "            # sub data set\n",
    "            # origfpath = f'{graph_split_file}/graph_patchesnames_{str(j)}{my_strategy}.csv'\n",
    "            # encodefpath = f'{graph_split_file}{my_strategy}_R_encode_coordinates_{j}.csv'\n",
    "            decodefpath = f'{graph_split_file}{my_strategy}_R_decode_coordinates_{j}.csv'\n",
    "\n",
    "            print(new_file_path, my_strategy, j)\n",
    "            \n",
    "            estimate_df = pd.read_csv(decodefpath)\n",
    "\n",
    "            # PerturbedList = perturbe_df[\"EncryPatchName\"].tolist()\n",
    "            # origList = orig_df[\"PatchName\"].tolist()\n",
    "            estList = estimate_df[\"DecryPatchName\"].tolist()\n",
    "            print(estList)\n",
    "\n",
    "            subcoordinates = [find_decrpytcoordinates(label) for label in estList]\n",
    "            print(subcoordinates)\n",
    "            coordinates.extend(subcoordinates)\n",
    "        de_coords.append(coordinates)\n",
    "\n",
    "\n",
    "# print(find_decrpytcoordinates(patchName))\n",
    "# print(find_encrpytcoordinates(patchName))\n",
    "print(len(de_coords), de_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create sub data sets with splitting algorithm\n",
    "from graph_based_splitting import graph_coloring_based_splitting, graph_coloring_based_splitting_visual, graph_coloring_based_splitting_visual1\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "\n",
    "# If ``strategy`` is a string, it must be one of the following,\n",
    "# each of which represents one of the built-in strategy functions.\n",
    "\n",
    "# * ``'largest_first'``\n",
    "# * ``'random_sequential'``\n",
    "# * ``'smallest_last'``\n",
    "# * ``'independent_set'``\n",
    "# * ``'connected_sequential_bfs'``\n",
    "# * ``'connected_sequential_dfs'``\n",
    "# * ``'connected_sequential'`` (alias for the previous strategy)\n",
    "# * ``'saturation_largest_first'``\n",
    "# * ``'DSATUR'`` (alias for the previous strategy)\n",
    "# graph_distribution = []\n",
    "# graph_split_file = os.mkdir(os.path.join(new_file_path, 'graph_split/'))\n",
    "\n",
    "\n",
    "\n",
    "for coordinates, new_file_path in zip(de_coords, new_file_paths): \n",
    "    graph_split_file = f'{new_file_path}/graph_split_de/'\n",
    "    if not os.path.exists(graph_split_file):\n",
    "        os.mkdir(graph_split_file)\n",
    "    \n",
    "    strategies = ['saturation_largest_first', 'largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "            'connected_sequential']\n",
    "    \n",
    "    # strategies = ['independent_set', 'connected_sequential']\n",
    "    for my_strategy in strategies: \n",
    "        # d, dataset_number = graph_coloring_based_splitting1(coordinates, patch_size, strategy=my_strategy)     # graph\n",
    "        d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "\n",
    "    print(\"---------------Well Done----------------\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. create sub data sets with splitting algorithm\n",
    "from graph_based_splitting import graph_coloring_based_splitting, graph_coloring_based_splitting_visual, graph_coloring_based_splitting_visual1\n",
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def worker(worker_id, graph_split_file, coordinates, patch_size, strategies): \n",
    "\n",
    "    for i, strategy in enumerate(strategies):\n",
    "        # with lock:\n",
    "            d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "    print(f\"Worker {worker_id} done.\")\n",
    "# If ``strategy`` is a string, it must be one of the following,\n",
    "# each of which represents one of the built-in strategy functions.\n",
    "\n",
    "# * ``'largest_first'``\n",
    "# * ``'random_sequential'``\n",
    "# * ``'smallest_last'``\n",
    "# * ``'independent_set'``\n",
    "# * ``'connected_sequential_bfs'``\n",
    "# * ``'connected_sequential_dfs'``\n",
    "# * ``'connected_sequential'`` (alias for the previous strategy)\n",
    "# * ``'saturation_largest_first'``\n",
    "# * ``'DSATUR'`` (alias for the previous strategy)\n",
    "# graph_distribution = []\n",
    "# graph_split_file = os.mkdir(os.path.join(new_file_path, 'graph_split/'))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # mp.set_start_method('spawn')  # Recommended for compatibility with PyTorch\n",
    "    \n",
    "\n",
    "    for coordinates, new_file_path in zip(de_coords, new_file_paths): \n",
    "        graph_split_file = f'{new_file_path}/graph_split_de/'\n",
    "        if not os.path.exists(graph_split_file):\n",
    "            os.mkdir(graph_split_file)\n",
    "        \n",
    "        strategies = ['saturation_largest_first', 'largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "                'connected_sequential']\n",
    "        \n",
    "        # Create a shared list (or array) to store results\n",
    "        # manager = mp.Manager()\n",
    "        # results = manager.list([0] * len(strategies))\n",
    "\n",
    "        # Create a lock to prevent simultaneous write access to the shared list\n",
    "        lock = manager.Lock()\n",
    "        \n",
    "        # Number of worker processes\n",
    "        num_workers = 12\n",
    "        # Create processes\n",
    "        processes = []\n",
    "        for i in range(num_workers):\n",
    "            worker_numbers = strategies[i::num_workers]\n",
    "            p = mp.Process(target=worker, args=(i, graph_split_file, coordinates, patch_size, strategies))\n",
    "            p.start()\n",
    "            processes.append(p)\n",
    "\n",
    "        # Join processes\n",
    "        for p in processes:\n",
    "            p.join()\n",
    "        \n",
    "        # print(f\"results: {list(results)}\")\n",
    "\n",
    "# for coordinates, new_file_path in zip(de_coords, new_file_paths): \n",
    "#     graph_split_file = f'{new_file_path}/graph_split_de/'\n",
    "#     if not os.path.exists(graph_split_file):\n",
    "#         os.mkdir(graph_split_file)\n",
    "    \n",
    "#     strategies = ['saturation_largest_first', 'largest_first', 'random_sequential', 'smallest_last', 'independent_set', \n",
    "#             'connected_sequential']\n",
    "    \n",
    "    # strategies = ['independent_set', 'connected_sequential']\n",
    "    # for my_strategy in strategies: \n",
    "    #     # d, dataset_number = graph_coloring_based_splitting1(coordinates, patch_size, strategy=my_strategy)     # graph\n",
    "    #     d, dataset_number = graph_coloring_based_splitting_visual1(graph_split_file, coordinates, patch_size, strategy=my_strategy)\n",
    "\n",
    "    #     # # Specify the directory path\n",
    "    #     # indexes = [item[0] for item in d.items()]\n",
    "    #     # patch_folder = f'{new_file_path}/patches/'\n",
    "    #     # patch_names = [os.path.join(patch_folder, patch) for patch in os.listdir(patch_folder)]\n",
    "\n",
    "    #     # # Create a DataFrame from the list of file names\n",
    "    #     # df = pd.DataFrame({\"Index\": indexes, \"PatchName\": patch_names})\n",
    "\n",
    "    #     # # Specify the CSV file path where the DataFrame will be saved\n",
    "    #     # csv_file_path = f'{new_file_path}/patch_names_{my_strategy}.csv'\n",
    "\n",
    "    #     # # Save the DataFrame to a CSV file\n",
    "    #     # df.to_csv(csv_file_path, index=False)\n",
    "    #     # print(f'File names have been saved to {csv_file_path}')\n",
    "\n",
    "    #     # OrgPatchName = df['PatchName']\n",
    "\n",
    "    #     # # store the split image names into different .csv files\n",
    "    #     # list = []\n",
    "    #     # for j in range(dataset_number):\n",
    "            \n",
    "    #     #     indexes = []\n",
    "    #     #     names = []\n",
    "    #     #     for item in d.items(): \n",
    "    #     #         # print(item) \n",
    "    #     #         if item[1] == j:\n",
    "    #     #             index = item[0]\n",
    "    #     #             # print(i, index, OrgPatchName[index])\n",
    "    #     #             indexes.append(index)\n",
    "    #     #             names.append(OrgPatchName[index])\n",
    "            \n",
    "    #     #     dataframe = pd.DataFrame({\"Index\": indexes, \"PatchName\": names})\n",
    "    #     #     destfp = os.path.join(graph_split_file, f'graph_patchesnames_'+str(j)+my_strategy+'.csv')\n",
    "    #     #     print(destfp)\n",
    "    #     #     # dataframe.to_csv('patchesnames_'+str(i)+'.csv', index=False, sep=',')\n",
    "    #     #     dataframe.to_csv(destfp, index=False, sep=',')\n",
    "\n",
    "    # print(\"---------------Well Done----------------\") "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
